{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c81169c30fb96244cfba0db287a0d3fd5325268de070a7a6116eecd4596df902"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IMDB 영화 리뷰 감성 분석\n",
    "- 테스트 데이터 25,000건중 10,000건은 Validation data로 활용\n",
    "- Conv1D + LSTM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "seed = 2021\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=None)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "index_dict = {}\n",
    "for key, value in imdb.get_word_index().items():\n",
    "    index_dict[value] = key\n",
    "len(index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"the as there in at by br of sure many br of proving no only women was than doesn't as you never of hat night that with ignored they bad out superman plays of how star so stories film comes defense date of wide they don't do that had with of hollywood br of my seeing fan this of pop out body shots in having because cause it's stick passing first were enjoys for from look seven sense from me superimposition die in character as cuban issues but is you that isn't one song just is him less are strongly not are you that different just even by this of you there is eight when it part are film's love film's 80's was big also light don't wrangling as it in character looked cinematography so stories is far br man acting\""
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "' '.join(index_dict[s] for s in X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "영화평 최대 길이: 2494\n영화평 평균 길이: 238.71364\n"
     ]
    }
   ],
   "source": [
    "print('영화평 최대 길이:', max(len(l) for l in X_train))\n",
    "print('영화평 평균 길이:', sum(map(len, X_train))/len(X_train))"
   ]
  },
  {
   "source": [
    "### Conv1D와 LSTM으로 IMDB 리뷰 감성 분류\n",
    "- 단어 빈도수: 5,000 (총 단어수: 88,584)\n",
    "- 문장의 단어수: 500단어 (최대: 2,494)\n",
    "- Test data중 10000개는 검증 데이터로"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.layers import Conv1D, Dropout, MaxPooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 빈도수: 5,000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 단어수: 500단어\n",
    "max_len = 500\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((15000, 500), (10000, 500), (15000,), (10000,))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Test data중 10000개는 검증 데이터로\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, stratify=y_test, test_size=0.4, random_state=seed\n",
    ")\n",
    "X_test.shape, X_val.shape, y_test.shape, y_val.shape"
   ]
  },
  {
   "source": [
    "### 모델 정의/설정/학습/평가"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, None, 120)         600000    \n_________________________________________________________________\ndropout (Dropout)            (None, None, 120)         0         \n_________________________________________________________________\nconv1d (Conv1D)              (None, None, 64)          38464     \n_________________________________________________________________\nmax_pooling1d (MaxPooling1D) (None, None, 64)          0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 60)                30000     \n_________________________________________________________________\ndense (Dense)                (None, 1)                 61        \n=================================================================\nTotal params: 668,525\nTrainable params: 668,525\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([ \n",
    "    Embedding(5000, 120),\n",
    "    Dropout(0.5),\n",
    "    Conv1D(64, 5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=4),\n",
    "    LSTM(60),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback 함수\n",
    "model_file = 'model/best_imdb_conv1d_lstm.h5'\n",
    "mc = ModelCheckpoint(model_file, save_best_only=True, verbose=1)\n",
    "es = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.31088, saving model to model\\best_imdb_conv1d_lstm.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.31088 to 0.27086, saving model to model\\best_imdb_conv1d_lstm.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.27086\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.27086\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.27086\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.27086\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.27086\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train, batch_size=100, epochs=50,\n",
    "    validation_data=(X_val, y_val), verbose=0, callbacks=[mc, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "469/469 [==============================] - 11s 24ms/step - loss: 0.2701 - accuracy: 0.8861\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.2700861096382141, 0.8860666751861572]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "best_model = load_model(model_file)\n",
    "best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}