{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c81169c30fb96244cfba0db287a0d3fd5325268de070a7a6116eecd4596df902"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IMDB 영화 리뷰 감성 분석"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "seed = 2021\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=None)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "index_dict = {}\n",
    "for key, value in imdb.get_word_index().items():\n",
    "    index_dict[value] = key\n",
    "len(index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "영화평 최대 길이: 2494\n영화평 평균 길이: 238.71364\n"
     ]
    }
   ],
   "source": [
    "print('영화평 최대 길이:', max(len(l) for l in X_train))\n",
    "print('영화평 평균 길이:', sum(map(len, X_train))/len(X_train))"
   ]
  },
  {
   "source": [
    "### LSTM으로 IMDB 리뷰 감성 분류\n",
    "- 단어 빈도수: 5,000 (총 단어수: 88,584)\n",
    "- 문장의 단어수: 500단어 (최대: 2,494)\n",
    "- Test data중 10000개는 검증 데이터로"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 빈도수: 5,000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 단어수: 500단어\n",
    "max_len = 500\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((15000, 500), (10000, 500), (15000,), (10000,))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Test data중 10000개는 검증 데이터로\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, stratify=y_test, test_size=0.4, random_state=seed\n",
    ")\n",
    "X_test.shape, X_val.shape, y_test.shape, y_val.shape"
   ]
  },
  {
   "source": [
    "### 모델 정의/설정/학습/평가"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, None, 120)         600000    \n_________________________________________________________________\nlstm (LSTM)                  (None, 120)               115680    \n_________________________________________________________________\ndense (Dense)                (None, 1)                 121       \n=================================================================\nTotal params: 715,801\nTrainable params: 715,801\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([ \n",
    "    Embedding(5000, 120),\n",
    "    LSTM(120),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback 함수\n",
    "import os\n",
    "if not os.path.exists('model'):\n",
    "    os.mkdir('model')\n",
    "model_file = 'model/best_imdb_lstm.h5'\n",
    "mc = ModelCheckpoint(model_file, save_best_only=True, verbose=1)\n",
    "es = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33313, saving model to model\\best_imdb_lstm.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33313\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33313 to 0.31620, saving model to model\\best_imdb_lstm.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.31620\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.31620\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.31620\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31620\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.31620\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train, batch_size=100, epochs=50,\n",
    "    validation_data=(X_val, y_val), verbose=0, callbacks=[mc, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "469/469 [==============================] - 45s 96ms/step - loss: 0.3164 - accuracy: 0.8707\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.31636741757392883, 0.8706666827201843]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "best_model = load_model(model_file)\n",
    "best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}